{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16056\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7c7ca112514d1e9d912ce7dac3d1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aec6631ca544f74bbd68b243d539f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/16056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c37b8feabdb4d33aae315619063e21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/31770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import Document, VectorStoreIndex\n",
    "import os, json\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#load documents\n",
    "documents = []\n",
    "directory = 'all_uva_news_articles_individual'\n",
    "for article in os.listdir(directory):\n",
    "    article_path = os.path.join(directory,article)\n",
    "    with open(article_path, \"r\",encoding = 'utf-8') as f:\n",
    "        article = json.loads(f.read())\n",
    "    content = article['text']\n",
    "    article.pop('text')\n",
    "    article.pop('url')\n",
    "\n",
    "    #SHORT TERM FIX, BETTER SOLUTION IS TO USE A LANGAUGE MODEL TO SUMMARARIZE THE ARTICLE/DESCRIPTION TO FIT IN A CERTAIN LENGTH\\\n",
    "    #FIRST CHECK THE AVERAGE/MEDIAN LENGTH OF DESCRIPTIONS AND HOW MANY DESCRIPTIONS ARE OVER 300/400 WORDS\n",
    "    description = article['description']\n",
    "    if len(description)>350:\n",
    "        article['description'] = description[:350]\n",
    "\n",
    "\n",
    "    doc = Document(text=content,metadata=article)\n",
    "    doc.id_ = article_path\n",
    "    documents.append(doc)\n",
    "\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "\n",
    "\n",
    "#build service context\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "#llm needs to be passed in for the embedding because of how the faiss vector store is implemented on llamaindex\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    #model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {'device': 'cpu'})\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024,llm=llm,embed_model=embed_model)\n",
    "\n",
    "\n",
    "#create nodes\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=128)\n",
    "nodes = node_parser.get_nodes_from_documents(documents=documents, show_progress=True)\n",
    "\n",
    "#building vector store index\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext, StorageContext\n",
    "import faiss #only have faiss-cpu installed for now, to get gpu:pip install faiss-gpu\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "service_context = ServiceContext.from_defaults(llm=llm,embed_model=embed_model)\n",
    "\n",
    "\n",
    "#build faiss index\n",
    "d = 768 \n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=faiss_vector_store)\n",
    "v_faiss_index = VectorStoreIndex(nodes=nodes, service_context=service_context, storage_context=storage_context,show_progress=True)\n",
    "#store faiss index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_faiss_index.storage_context.persist(\"./faiss_vector_store\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
