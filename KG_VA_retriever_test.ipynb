{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build service context\n",
    "#build service context for querying\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "\n",
    "query_llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs= {'offload_folder': \"offload\"}\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    #model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {'device': 'cpu'})\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024,llm=query_llm,embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "from llama_index import ServiceContext, StorageContext, load_index_from_storage, load_graph_from_storage\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from KG_VA_retriever import KG_VA_retreiver\n",
    "from llama_index.retrievers import VectorIndexRetriever, KGTableRetriever\n",
    "from llama_index import MockEmbedding\n",
    "from llama_index.llms import MockLLM\n",
    "\n",
    "#load_service_context = ServiceContext.from_defaults(llm=MockLLM(),embed_model=MockEmbedding(768))\n",
    "\n",
    "#load indicies from storage\n",
    "#build faiss index - using just cpu for now, will change\n",
    "faiss_vector_store = FaissVectorStore.from_persist_dir(\"./faiss_vector_store\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=faiss_vector_store, persist_dir=\"./faiss_vector_store\"\n",
    ")\n",
    "fiass_index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "#build kg index\n",
    "graph_store = SimpleGraphStore.from_persist_dir(\"./knowledge_graph_store\")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store,\n",
    "                                               persist_dir='./knowledge_graph_store')\n",
    "kg_graph = load_index_from_storage(storage_context=storage_context, service_context = service_context)\n",
    "\n",
    "\n",
    "\n",
    "#create retrievers\n",
    "v_retriever = VectorIndexRetriever(\n",
    "    index=fiass_index,\n",
    "    similarity_top_k=3,\n",
    "    vector_store_query_mode=\"default\",\n",
    "    alpha=None,\n",
    "    doc_ids=None,\n",
    ") #https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/vector_store_guide.html\n",
    "\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_graph, \n",
    "    retriever_mode=\"hybrid\", \n",
    "    include_text=True, \n",
    "    similarity_top_k= 3,\n",
    "    use_global_node_triplets=True\n",
    ")\n",
    "\n",
    "custom_retriever = KG_VA_retreiver(v_retriever,kg_retriever, mode='OR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode='compact', service_context=service_context)\n",
    "\n",
    "\n",
    "#build query engine\n",
    "query_engine = RetrieverQueryEngine(retriever = custom_retriever,response_synthesizer=response_synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What McIntire student wrote for Key&Peele?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
