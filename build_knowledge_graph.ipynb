{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3476f835cd4e1297331ad897746425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 113632d0-c238-4c0e-932e-16075689ecc0)')' thrown while requesting HEAD https://huggingface.co/Babelscape/rebel-large/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574b04e92a0d4fe7abfa118ffed7e49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d75b7b3921491aaa60efeeddbfe553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a95261e0ef443db988b28cb98258711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189448f76849429b96cf3b2d051d036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d39a0150a5c4f579282ec27a664f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f31aa250f2345a79c570aa047159c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b8fa2e119a477e8fdb52a16a459059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/344 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract kg triplets from data\n",
    "#code from Babelscape/rebel-large model card\n",
    "#https://huggingface.co/Babelscape/rebel-large\n",
    "\n",
    "def extract_triplets(input_text):\n",
    "    text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(input_text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])[0]\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "import os\n",
    "import json\n",
    "def build_documents(directory):\n",
    "  documents = []\n",
    "  for article in os.listdir(directory):\n",
    "      article_path = os.path.join(directory,article)\n",
    "      with open(article_path, \"r\",encoding = 'utf-8') as f:\n",
    "          article = json.loads(f.read())\n",
    "      content = article['text']\n",
    "      article.pop('text')\n",
    "      article.pop('url')\n",
    "\n",
    "      #this is a very rare occurance (happens once as of 8/11) so this quick fix is fine\n",
    "      description = article['description']\n",
    "      if len(description)>350:\n",
    "          article['description'] = description[:350]\n",
    "\n",
    "\n",
    "      doc = Document(text=content,metadata=article)\n",
    "      doc.id_ = article_path\n",
    "      documents.append(doc)\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "def build_nodes(documents, chunk_size, chunk_overlap):\n",
    "    node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents=documents, show_progress=True)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m query_wrapper_prompt \u001b[39m=\u001b[39m SimpleInputPrompt(\u001b[39m\"\u001b[39m\u001b[39m<|USER|>\u001b[39m\u001b[39m{query_str}\u001b[39;00m\u001b[39m<|ASSISTANT|>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39m#llm needs to be passed in for the embedding because of how the faiss vector store is implemented on llamaindex\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m llm \u001b[39m=\u001b[39m HuggingFaceLLM(\n\u001b[0;32m     20\u001b[0m     context_window\u001b[39m=\u001b[39;49m\u001b[39m4096\u001b[39;49m,\n\u001b[0;32m     21\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[0;32m     22\u001b[0m     generate_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.7\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdo_sample\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m},\n\u001b[0;32m     23\u001b[0m     system_prompt\u001b[39m=\u001b[39;49msystem_prompt,\n\u001b[0;32m     24\u001b[0m     query_wrapper_prompt\u001b[39m=\u001b[39;49mquery_wrapper_prompt,\n\u001b[0;32m     25\u001b[0m     tokenizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mStabilityAI/stablelm-tuned-alpha-3b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mStabilityAI/stablelm-tuned-alpha-3b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     27\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     28\u001b[0m     stopping_ids\u001b[39m=\u001b[39;49m[\u001b[39m50278\u001b[39;49m, \u001b[39m50279\u001b[39;49m, \u001b[39m50277\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m],\n\u001b[0;32m     29\u001b[0m     tokenizer_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m4096\u001b[39;49m},\n\u001b[0;32m     30\u001b[0m     \u001b[39m# uncomment this if using CUDA to reduce memory usage\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m     \u001b[39m#model_kwargs={\"torch_dtype\": torch.float16}\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     35\u001b[0m embed_model \u001b[39m=\u001b[39m LangchainEmbedding(\n\u001b[0;32m     36\u001b[0m   HuggingFaceEmbeddings(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[39m\"\u001b[39m, model_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39mfrom_defaults(chunk_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,llm\u001b[39m=\u001b[39mllm,embed_model\u001b[39m=\u001b[39membed_model)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\llama_index\\llms\\huggingface.py:51\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[1;34m(self, context_window, max_new_tokens, system_prompt, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, callback_manager)\u001b[0m\n\u001b[0;32m     49\u001b[0m model_kwargs \u001b[39m=\u001b[39m model_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_name \u001b[39m=\u001b[39m model_name\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     52\u001b[0m     model_name, device_map\u001b[39m=\u001b[39mdevice_map, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[39m# check context_window\u001b[39;00m\n\u001b[0;32m     56\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    495\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:2903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2893\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2894\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   2896\u001b[0m     (\n\u001b[0;32m   2897\u001b[0m         model,\n\u001b[0;32m   2898\u001b[0m         missing_keys,\n\u001b[0;32m   2899\u001b[0m         unexpected_keys,\n\u001b[0;32m   2900\u001b[0m         mismatched_keys,\n\u001b[0;32m   2901\u001b[0m         offload_index,\n\u001b[0;32m   2902\u001b[0m         error_msgs,\n\u001b[1;32m-> 2903\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   2904\u001b[0m         model,\n\u001b[0;32m   2905\u001b[0m         state_dict,\n\u001b[0;32m   2906\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   2907\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2908\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2909\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   2910\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   2911\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   2912\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   2913\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   2914\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   2915\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   2916\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   2917\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[0;32m   2918\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   2919\u001b[0m     )\n\u001b[0;32m   2921\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[0;32m   2922\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:3002\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3000\u001b[0m is_safetensors \u001b[39m=\u001b[39m archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3001\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_safetensors:\n\u001b[1;32m-> 3002\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3003\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3004\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m for them. Alternatively, make sure you have `safetensors` installed if the model you are using\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3005\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m offers the weights in this format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3006\u001b[0m     )\n\u001b[0;32m   3007\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3008\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
     ]
    }
   ],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "#build service context\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "#llm needs to be passed in for the embedding because of how the faiss vector store is implemented on llamaindex\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    offload_folder = 'offload',\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    #model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {'device': 'cpu'})\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512,llm=llm,embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build knowledge graph index\n",
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "documents = build_documents('all_uva_news_articles_individual')\n",
    "nodes = build_nodes(documents,512,128)\n",
    "\n",
    "\n",
    "index = KnowledgeGraphIndex(nodes=nodes, kg_triplet_extract_fn=extract_triplets, service_context=service_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
