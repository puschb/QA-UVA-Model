{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llama_index import Document, VectorStoreIndex\n",
    "import os, json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#load documents\n",
    "documents = []\n",
    "directory = 'uva_articles_sample'\n",
    "for article in os.listdir(directory):\n",
    "    article_path = os.path.join(directory,article)\n",
    "    with open(article_path, \"r\",encoding = 'utf-8') as f:\n",
    "        article = json.loads(f.read())\n",
    "    content = article['text']\n",
    "    article.pop('text')\n",
    "    article.pop('url')\n",
    "    doc = Document(text=content,metadata=article)\n",
    "    doc.id_ = article_path\n",
    "    documents.append(doc)\n",
    "\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#things to do\n",
    "# test metadata extraction with 10 documents, maybe make a custom one\n",
    "# implement meta data extraction by creating nodes from the documents\n",
    "# once the nodes are created (could take a brick), turn the nodes into indicies using the different index methods\n",
    "#vector db to use: FAISS\\\n",
    "#chunk size = 1024\n",
    "#chunk overlap = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Users\\Benjamin\\Documents\\Random Projects\\UVA-QA-Model\\QA-UVA-Model\\.venv\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490939b3e49443e5827273cddab563ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#build service context for querying\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "\n",
    "query_llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    #model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {'device': 'cpu'})\n",
    ")\n",
    "\n",
    "query_service_context = ServiceContext.from_defaults(chunk_size=1024,llm=query_llm,embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e82c6a9f874e719db46d74356a4c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embed_model = LangchainEmbedding(\\n  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {\\'device\\': \\'cpu\\'})\\n)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load llm and embed models\n",
    "from llama_index.llms import MockLLM\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "llm = MockLLM()\n",
    "'''embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs = {'device': 'cpu'})\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d = 768 \\nfaiss_index = faiss.IndexFlatL2(d)\\nfaiss_vector_store = FaissVectorStore(faiss_index=faiss_index)\\nstorage_context = StorageContext.from_defaults(vector_store=faiss_vector_store)\\n\\nv_faiss_index = VectorStoreIndex(nodes=nodes, service_context=query_service_context, storage_context=storage_context,show_progress=True)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building vector store index\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext, StorageContext\n",
    "import faiss #only have faiss-cpu installed for now, to get gpu:pip install faiss-gpu\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "service_context = ServiceContext.from_defaults(llm=llm,embed_model=embed_model)\n",
    "\n",
    "#v_index = VectorStoreIndex(nodes=nodes,service_context=service_context,show_progress=True)\n",
    "\n",
    "'''d = 768 \n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=faiss_vector_store)\n",
    "\n",
    "v_faiss_index = VectorStoreIndex(nodes=nodes, service_context=query_service_context, storage_context=storage_context,show_progress=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building query engine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode='compact', service_context=query_service_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=v_faiss_index,\n",
    "    similarity_top_k=3,\n",
    "    vector_store_query_mode=\"default\",\n",
    "    alpha=None,\n",
    "    doc_ids=None,\n",
    ") #https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/vector_store_guide.html\n",
    "\n",
    "# build query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "query_engine_faiss = v_faiss_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#query\n",
    "#response = query_engine.query(\"What did the Dave Matthews band do?\")\n",
    "response = query_engine_faiss.query(\"What did the Dave Matthews band do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dave Matthews Band is a student-run radio station in Virginia that plays a variety of unique songs and has been providing music education and community outreach for over two decades. The band has produced numerous notable performances, including the 17 Blockbusters Arts Highlights from 2017. The station has also hosted many prestigious events, including the Bicentennial Launch Celebration and the Concert for Charlottesville. The band has also produced several successful albums and has been recognized with numerous awards and accolades.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "#display(Markdown(f\"<b>{response}</b>\"))\n",
    "with open(\"source_nodes_text.txt\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(response.get_formatted_sources(length=10000))\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss test\n",
    "#works better than regular vector store\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "documents = SimpleDirectoryReader(\"./uva_articles_sample\").load_data()\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context,service_context=query_service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the Dave Matthews band do?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aa628f8b6846229fb135e1383b4c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting keywords from nodes:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Benjamin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#tree index test\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import KeywordTableIndex\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "g_store_index = VectorStoreIndex(nodes=nodes, service_context=query_service_context, storage_context=storage_context,show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "title: 2 Alumnae to Continue Studies as Knight-Hennessy Scholars\n",
      "author: Matt Kelly\n",
      "date: 5/9/2022 6:19:37 PM\n",
      "category: University News\n",
      "description: Two University of Virginia graduates will delve into Black genre literature, and public and business administration as Knight-Hennessy Scholars.\n",
      "\n",
      "Two University of Virginia alumnae will pursue their graduate studies at Stanford University as Knight-Hennessy scholars., Kristen “Kay” Barrett, a 2020 English graduate, will pursue a doctorate in English literature. Katie Deal, a 2017 political and social thought graduate, will pursue master’s degrees in public administration and business administration., Knight-Hennessy Scholars receive up to three years of funding to pursue graduate study at Stanford, as well as joint- and dual-degrees., The awards are funded by Stanford alumnus Phil Knight, a philanthropist and co-founder of Nike Inc., along with other benefactors. The Knight-Hennessy Scholarship Program is named for Knight and John Hennessy, who served as Stanford University’s 10th president from 2000 to 2016., The program “is an incredible community of young folks looking to make a mark through curiosity, hard work and leadership,” said Andrus G. Ashoo, director of the Office of Citizen Scholar Development. “It has been a real treat to work with Katie and Kay as alumni. They will be a perfect complement to the rest of the scholar community, and I look forward to remaining in touch and hearing stories from their interactions with the other UVA alumni who are currently at Stanford with Knight-Hennessy.”, Kristen “Kay” Barrett, Kristen “Kay” Barrett will investigate Black speculative fiction in novels and contemporary film. (Contributed photo), Barrett, of Nashville, Tennessee, says the Knight-Hennessy scholarship gives her a license to dream., Barrett graduated from UVA in May 2020 as a distinguished major in English with a drama minor, as well as being a Jefferson Scholar. As a Marshall Scholar, she graduated from the University of Oxford with a Master of Studies in 19th-century English, and from the University of Edinburgh with an Master of Science degree in Intermediality: Literature, Film and the Arts in Dialogue., “The Knight-Hennessy will empower me to chase my ambitions both inside and outside of academia. As a Ph.D. candidate in English literature, I will investigate Black science fiction in novels and contemporary film,” Barrett said. “My work advocates for a tradition of speculative thought that connects the Reconstruction Era to our current times. Alongside this research, I will also dive into the publishing industry, carving out a space for Black authors of science, horror, fantasy and mystery.”, In speculative fiction, Barrett said, Black creators can imagine utopian futures and alternate histories., “Unlike mainstream science fiction, Black sci-fi taps into the essence of the genre because members of the African diaspora write from their own dystopian reality,” she said. “What’s more dystopian than living in a world where you’re judged by the color of your skin?”, Science fiction, she added, “does not receive the same literary attention and respect as the classics or literary fiction. I am intrigued by how this genre offers an intellectual and creative refuge for Black creatives.”, Stephen Cushman, the Robert C. Taylor Professor in the Department of English, described Barrett as a “bright light.”, “I did not learn of most of her myriad accomplishments until I pushed her for a fuller picture of her endeavors,” he said. “She was always deeply reluctant to blow her own horn or enumerate her accolades. Throughout her UVA years, Kristen was courageous, dedicated and generous. She embodied then and still embodies a rare combination of social engagement, selfless service, intellectual curiosity and artistic vision.”, Barrett said the intense isolation she endured during the pandemic has reinvigorated her love for people., “I find myself\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the question: What did the Dave Matthews band do?\n",
      "\n",
      "[NodeWithScore(node=TextNode(id_='ced379fa-d97a-41b1-a3b4-0abd13b075a1', embedding=None, metadata={'title': '2 Alumnae to Continue Studies as Knight-Hennessy Scholars', 'author': 'Matt Kelly', 'date': '5/9/2022 6:19:37 PM', 'category': 'University News', 'description': 'Two University of Virginia graduates will delve into Black genre literature, and public and business administration as Knight-Hennessy Scholars.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='uva_articles_sample\\\\_Alumnae_to_Continue_Studies_as_KnightHennessy_Scholars.json', node_type=None, metadata={'title': '2 Alumnae to Continue Studies as Knight-Hennessy Scholars', 'author': 'Matt Kelly', 'date': '5/9/2022 6:19:37 PM', 'category': 'University News', 'description': 'Two University of Virginia graduates will delve into Black genre literature, and public and business administration as Knight-Hennessy Scholars.'}, hash='1fafa4fb855250c4d2cbac862824c5a0e55cd12e735c329a08f697f9ffe4270a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ab301772-7cef-4603-bcae-4a4ac486adb4', node_type=None, metadata={'title': '2 Alumnae to Continue Studies as Knight-Hennessy Scholars', 'author': 'Matt Kelly', 'date': '5/9/2022 6:19:37 PM', 'category': 'University News', 'description': 'Two University of Virginia graduates will delve into Black genre literature, and public and business administration as Knight-Hennessy Scholars.'}, hash='6a9625090d087d8a1dd65aaa085dd1c74a6916a2dc28c367aa9fe113692b15fb')}, hash='b279089fa0e3ad0c2a23330bba1b043d88e03ab08c32ef819111b71ff606e977', text='Two University of Virginia alumnae will pursue their graduate studies at Stanford University as Knight-Hennessy scholars., Kristen “Kay” Barrett, a 2020 English graduate, will pursue a doctorate in English literature. Katie Deal, a 2017 political and social thought graduate, will pursue master’s degrees in public administration and business administration., Knight-Hennessy Scholars receive up to three years of funding to pursue graduate study at Stanford, as well as joint- and dual-degrees., The awards are funded by Stanford alumnus Phil Knight, a philanthropist and co-founder of Nike Inc., along with other benefactors. The Knight-Hennessy Scholarship Program is named for Knight and John Hennessy, who served as Stanford University’s 10th president from 2000 to 2016., The program “is an incredible community of young folks looking to make a mark through curiosity, hard work and leadership,” said Andrus G. Ashoo, director of the Office of Citizen Scholar Development. “It has been a real treat to work with Katie and Kay as alumni. They will be a perfect complement to the rest of the scholar community, and I look forward to remaining in touch and hearing stories from their interactions with the other UVA alumni who are currently at Stanford with Knight-Hennessy.”, Kristen “Kay” Barrett, Kristen “Kay” Barrett will investigate Black speculative fiction in novels and contemporary film. (Contributed photo), Barrett, of Nashville, Tennessee, says the Knight-Hennessy scholarship gives her a license to dream., Barrett graduated from UVA in May 2020 as a distinguished major in English with a drama minor, as well as being a Jefferson Scholar. As a Marshall Scholar, she graduated from the University of Oxford with a Master of Studies in 19th-century English, and from the University of Edinburgh with an Master of Science degree in Intermediality: Literature, Film and the Arts in Dialogue., “The Knight-Hennessy will empower me to chase my ambitions both inside and outside of academia. As a Ph.D. candidate in English literature, I will investigate Black science fiction in novels and contemporary film,” Barrett said. “My work advocates for a tradition of speculative thought that connects the Reconstruction Era to our current times. Alongside this research, I will also dive into the publishing industry, carving out a space for Black authors of science, horror, fantasy and mystery.”, In speculative fiction, Barrett said, Black creators can imagine utopian futures and alternate histories., “Unlike mainstream science fiction, Black sci-fi taps into the essence of the genre because members of the African diaspora write from their own dystopian reality,” she said. “What’s more dystopian than living in a world where you’re judged by the color of your skin?”, Science fiction, she added, “does not receive the same literary attention and respect as the classics or literary fiction. I am intrigued by how this genre offers an intellectual and creative refuge for Black creatives.”, Stephen Cushman, the Robert C. Taylor Professor in the Department of English, described Barrett as a “bright light.”, “I did not learn of most of her myriad accomplishments until I pushed her for a fuller picture of her endeavors,” he said. “She was always deeply reluctant to blow her own horn or enumerate her accolades. Throughout her UVA years, Kristen was courageous, dedicated and generous. She embodied then and still embodies a rare combination of social engagement, selfless service, intellectual curiosity and artistic vision.”, Barrett said the intense isolation she endured during the pandemic has reinvigorated her love for people., “I find myself', start_char_idx=0, end_char_idx=3665, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)]\n"
     ]
    }
   ],
   "source": [
    "g_query_engine = g_store_index.as_query_engine()\n",
    "response = g_query_engine.query(\"What did the Dave Matthews band do?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n",
    "print(response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
